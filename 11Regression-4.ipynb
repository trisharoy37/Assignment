{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "Ans: \n",
    "Lasso Regression is a linear regression technique that incorporates a regularization term to prevent overfitting. It's particularly useful when dealing with a large number of features, as it can automatically perform feature selection by setting some coefficients to zero.\n",
    "\n",
    "Key Difference from Other Regression Techniques:\n",
    "\n",
    "- Feature Selection: Unlike other regression techniques like Ordinary Least Squares (OLS) or Ridge Regression, Lasso Regression explicitly performs feature selection. By adding an L1 penalty term to the loss function, it encourages the model to shrink the coefficients of less important features towards zero.   \n",
    "- Sparsity: This feature selection property leads to sparse models, where only a subset of the features is used to make predictions.   \n",
    "- Interpretability: Sparse models are often more interpretable, as they focus on the most relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "Ans: \n",
    "The primary advantage of using Lasso Regression for feature selection is it's ability to automatically identify and eliminate irrelevant features.\n",
    "By adding an L1 penalty term to the loss function, Loss regression encourages some coefficients to become exactly zero. This effectively removes these features from the model, resulting in a simpler and more interpretable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "Ans:\n",
    "Interpreting coefficients in Lasso Regression is similar to interpreting coefficients in ordinary least squares (OLS) regression, with a key difference: Lasso Regression can shrink some coefficients to exactly zero.   \n",
    "\n",
    "Here's how to interpret the coefficients:\n",
    "\n",
    "- Non-Zero Coefficients:\n",
    "For non-zero coefficients, the interpretation is similar to OLS: a one-unit increase in the independent variable is associated with a change of the coefficient value in the dependent variable, holding all other variables constant.\n",
    "- Zero Coefficients:\n",
    "If a coefficient is zero, it means that the corresponding feature is not considered important by the model and has been effectively removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?\n",
    "\n",
    "\n",
    "Ans:\n",
    "The primary tuning parameter in Lasso Regression is the regularization parameter (λ). This parameter controls the strength of the L1 penalty term, which in turn determines the degree of feature selection and shrinkage of coefficients.   \n",
    "\n",
    "Effect of λ on Model Performance:\n",
    "- Small λ:\n",
    "Less regularization.\n",
    "Model tends towards OLS regression.\n",
    "More features may be included, potentially leading to overfitting.\n",
    "\n",
    "- Large λ:\n",
    "Strong regularization.\n",
    "More features may be excluded, potentially leading to underfitting.\n",
    "Simpler model with fewer coefficients.   \n",
    "\n",
    "Choosing the Optimal λ:\n",
    "The optimal value of λ depends on the specific dataset and the desired trade-off between bias and variance. Techniques like:   \n",
    "- Cross-validation: This involves splitting the data into multiple folds, training the model on a subset of the folds, and evaluating its performance on the remaining fold for different values of λ. The value of λ that minimizes the average error across all folds is chosen.   \n",
    "- Information Criteria: Methods like Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) can be used to select the optimal λ by penalizing model complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "Ans: \n",
    "Yes, Lasso Regression can be used for non-linear regression problems, but indirectly.   \n",
    "\n",
    "While Lasso Regression itself is a linear model, it can be combined with techniques that can introduce non-linearity into the model:\n",
    "\n",
    "1. Polynomial Regression:\n",
    "By creating polynomial features (e.g., squared, cubed terms) from the original features, we can introduce non-linear relationships.   \n",
    "Lasso can then be applied to this transformed dataset to select the most relevant polynomial terms.\n",
    "\n",
    "2. Feature Engineering:\n",
    "Create non-linear transformations of the features, such as logarithmic, exponential, or trigonometric functions.\n",
    "Lasso can then be used to select the most important transformed features.\n",
    "\n",
    "3. Kernel Methods:\n",
    "Kernel methods, like Kernel Ridge Regression, can implicitly map data into a higher-dimensional space, where linear relationships may exist.   \n",
    "Lasso can be used to select the most important features in this higher-dimensional space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({\n",
    "    \"Feature\": (\"Regularization\",\"Coefficient Shrinkage\" , \"Feature Selection\", \"Model Complexity\",\"Multicollinearity Handling\")\n",
    "    ,\"Ridge Regression\":(\"L2 Norm\",\" Shrinks towards zero\",\"No explicit feature selection\",\"Less sparse models\",\"Effective\")\n",
    "    ,\" Lasso Regression\":(\"L1 Norm\",\"Sets some coefficients to zero\",\"Performs feature selection\",\"More sparse models\",\"Less effective\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Ridge Regression</th>\n",
       "      <th>Lasso Regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regularization</td>\n",
       "      <td>L2 Norm</td>\n",
       "      <td>L1 Norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coefficient Shrinkage</td>\n",
       "      <td>Shrinks towards zero</td>\n",
       "      <td>Sets some coefficients to zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feature Selection</td>\n",
       "      <td>No explicit feature selection</td>\n",
       "      <td>Performs feature selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model Complexity</td>\n",
       "      <td>Less sparse models</td>\n",
       "      <td>More sparse models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Multicollinearity Handling</td>\n",
       "      <td>Effective</td>\n",
       "      <td>Less effective</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Feature               Ridge Regression  \\\n",
       "0              Regularization                        L2 Norm   \n",
       "1       Coefficient Shrinkage           Shrinks towards zero   \n",
       "2           Feature Selection  No explicit feature selection   \n",
       "3            Model Complexity             Less sparse models   \n",
       "4  Multicollinearity Handling                      Effective   \n",
       "\n",
       "                 Lasso Regression  \n",
       "0                         L1 Norm  \n",
       "1  Sets some coefficients to zero  \n",
       "2      Performs feature selection  \n",
       "3              More sparse models  \n",
       "4                  Less effective  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "Ans:\n",
    "Yes, Lasso Regression can handle multicollinearity in input features.   \n",
    "\n",
    "While it doesn't directly address multicollinearity like Ridge Regression, it can mitigate its effects through its feature selection property. Here's how:   \n",
    "\n",
    "1. Feature Selection: Lasso Regression tends to select one feature from a group of highly correlated features. This can help to reduce the impact of multicollinearity, as the model focuses on the most informative feature.   \n",
    "\n",
    "2. Coefficient Shrinkage: Lasso shrinks the coefficients of less important features towards zero. This can help to stabilize the model and reduce the impact of noise and multicollinearity.   \n",
    "\n",
    "However, it's important to note that Lasso Regression may not be the best choice for all cases of multicollinearity. In severe cases of multicollinearity, Ridge Regression or Elastic Net Regression (a combination of Ridge and Lasso) might be more suitable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "Ans:\n",
    "- Grid Search: Create a range of λ values and evaluate the model performance for each value using cross-validation or information criteria.\n",
    "- Regularization Path: Visualize the model coefficients as a function of λ to understand the impact of different values.\n",
    "- Domain Knowledge: Consider the specific problem and the expected level of regularization.\n",
    "- Model Selection: Use appropriate metrics like MSE, RMSE, or R-squared to assess model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
