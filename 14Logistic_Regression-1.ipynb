{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q1. Explain the difference between linear regression and logistic regression models. Provide an example of\n",
    "a scenario where logistic regression would be more appropriate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Ans:  Linear Regression:\n",
    "- Predicts a continuous outcome(e.g., house price, temperatue).\n",
    "- Models a linear relationship between variables.\n",
    "- Best fit line is used to make predictions.\n",
    "\n",
    "Logistic Regression:\n",
    "- Predicts a categorical outcome(e.g., yes/no, spam/not spam)\n",
    "- Models the probability of a certain outcome.\n",
    "- S-shaped curve (sigmoid function) is used to make predictions\n",
    "\n",
    "Scenario:\n",
    "A healthcare organization wants to predict whether a patient is likely to develop diabetes (Yes/No) based on features such as age, BMI, glucose levels, and family history.\n",
    "\n",
    "Why Logistic Regression is Appropriate:\n",
    "- Nature of the Problem: The outcome(diabetes or no diabetes) is binary (categorical).\n",
    "- Output: Logistic regression predicts the probability of developing diabetes.\n",
    "- Interpretability: The model provides odds ratios, which are useful for understanding the effect of each feature on the likelihood of diabetes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q2. What is the cost function used in logistic regression, and how is it optimized?**\n",
    "\n",
    "Ans: The cost function in logistic regression is called log loss (or cross-entropy loss). It measures the difference between predicted probabilities and actual outcomes.\n",
    "\n",
    "It's optimized using gradient descent, an iterative algorithm that finds the parameters minimizing the cost function by repeatedly adjusting them in the direction of the steepest decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.**\n",
    "\n",
    "Ans: Regularization in logistic regression is a technique used to prevent overfitting, which occurs when a model learns the training data too well, including its noise and outliers, leading to poor performance on new, unseen data. Regularization achieves this by adding a penalty term to the cost function that the model tries to minimize during training. This penalty discourages the model from assigning excessively large coefficients to the features. Regularization in logistic regression acts as a constraint on the model's complexity, preventing it from becoming too specialized to the training data and improving its ability to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression\n",
    "model?**\n",
    "\n",
    "Ans:\n",
    "The ROC(Receiver Operating Characteristic) curve is a graphical representation of the performance of a binary classification model, like logistic regression, at all possible classification thresholds. It plots two key metrics:\n",
    "- True Positive Rate(TPR) or Sensitivity: The proportion of actual positives that are correctly identified.\n",
    "- False Positive Rate(FPR): The proportion of actual negatives that are incorrectly identified as positives.\n",
    "\n",
    "How it's used to evaluate logistic regression:\n",
    "\n",
    "Varying the Threshold: Logistic regression outputs probabilities between 0 and 1. To make a final classification (e.g., yes/no), you need to set a threshold (e.g., 0.5). The ROC curve is created by varying this threshold and calculating the TPR and FPR at each threshold.   \n",
    "\n",
    "Plotting the Curve: The TPR is plotted on the y-axis, and the FPR is plotted on the x-axis. This creates a curve that shows the trade-off between sensitivity and specificity (1 - FPR) at different thresholds.   \n",
    "\n",
    "Ideal Curve: An ideal classifier would have a curve that goes straight up to the top left corner (TPR = 1, FPR = 0), meaning it correctly classifies all instances.\n",
    "\n",
    " Area Under the Curve (AUC): The AUC is a single number that summarizes the overall performance of the model. An AUC of 1 represents a perfect classifier, while an AUC of 0.5 represents a classifier that performs no better than random guessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q5. What are some common techniques for feature selection in logistic regression? How do these\n",
    "techniques help improve the model's performance?**\n",
    "\n",
    "Feature selection in logistic regression aims to identify the most relevant features for predicting the outcome, discarding irrelevant or redundant ones. This simplifies the model, improves performance, and reduce overfitting.\n",
    "Here are some common techniques:\n",
    "- Filter Methods: Use statistical measure to rank features.Examples include:\n",
    "    - Chi-square test:Measures the independence between categorical features and outcome.\n",
    "    - ANOVA: Analyzes variance between groups to assess feature relevance\n",
    "\n",
    "- Wrapper Methods: Evaluate feature subsets by training the model with different combinations. Examples include:\n",
    "    - Forward selection: Starts with an empty set and adds the best feature iteratively.   \n",
    "    - Backward elimination: Starts with all features and removes the least important one iteratively.\n",
    "    - Recursive Feature Elimination (RFE): Recursively removes features based on their importance ranking\n",
    "\n",
    "- Embedded Methods: Perform feature selection as part of the model training process. Examples include:\n",
    "    - L1 regularization (Lasso): Shrinks some feature coefficients to zero, effectively selecting features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing\n",
    "with class imbalance?**\n",
    "\n",
    "Ans:\n",
    "Imbalanced datasets, where one class has significantly more instances than the other, can bias logistic regression towards the majority class. Here's how to handle them:   \n",
    "\n",
    "1. Data-level techniques:\n",
    "\n",
    "    - Oversampling: Increase the number of minority class instances by:\n",
    "        - Random oversampling: Duplicating existing instances.   \n",
    "        - SMOTE (Synthetic Minority Over-sampling Technique): Creating synthetic instances based on existing ones.   \n",
    "    - Undersampling: Decrease the number of majority class instances by:\n",
    "        - Random undersampling: Randomly removing instances.   \n",
    "        - Tomek links: Removing majority class instances that are \"close\" to minority class instances.   \n",
    "2. Algorithm-level techniques:\n",
    "\n",
    "- Class weights: Assign higher weights to the minority class in the cost function, making misclassifications more costly.   \n",
    "3. Evaluation metrics:\n",
    "\n",
    "Use metrics beyond accuracy, such as:\n",
    "- Precision: Proportion of true positives among predicted positives.   \n",
    "- Recall (Sensitivity): Proportion of true positives among actual positives.   \n",
    "- F1-score: Harmonic mean of precision and recall.\n",
    "- AUC: Area under the ROC curve.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q7. Can you discuss some common issues and challenges that may arise when implementing logistic\n",
    "regression, and how they can be addressed? For example, what can be done if there is multicollinearity\n",
    "among the independent variables?**\n",
    "\n",
    "Ans:\n",
    "Common issues in logistic regression and their solutions:\n",
    "- Multicollinearity(high correlation between predictors):\n",
    "    - Problem: Makes it hard to isolate the effect of individual predictors, inflates standard errors.\n",
    "    - Solution: Remove one of the correlated variables, combine them into a composite variable, or use regularization(L1 or L2).\n",
    "- Overfitting:\n",
    "    - Problem: Model performs well on training data but poorly on unseen data.\n",
    "    - Solution: Use regularization, cross-validation , or simplify the model(feature selection).\n",
    "- Outliers:\n",
    "    - Problem: Can disproportionately influence the model.\n",
    "    - Solution: Identify and remove or transform outliers.\n",
    "- Imbalanced datasets:\n",
    "    - Problem: Model biased towards the majority class.\n",
    "    - Solution: Use oversampling, undersampling, class weights, or appropriate evaluation metrics(precision,recall,F1-score,AUC).\n",
    "\n",
    "- Linearity assumption violation(for continuous predictors):\n",
    "    - Problem:Logistic regression assumes a linear relationship between continuous predictors and the log-odds of the outcome.\n",
    "    - Solution: Transform the predictor variables or use a non-linear model.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
