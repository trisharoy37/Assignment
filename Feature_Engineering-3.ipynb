{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how it is used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution:\n",
    "Min-max scaling, also known as normalization, is a data preprocessing technique used to transform the values of numeric data to a specific range, usually between 0 and 1.This scaling is particularly useful when you need to ensure that all features contribute equally to the learning process in machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  ],\n",
       "       [0.25],\n",
       "       [0.5 ],\n",
       "       [0.75],\n",
       "       [1.  ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##for example\n",
    "import pandas as pd\n",
    "df=pd.DataFrame({'person':['A','B','C','D','E'],'Height':[150,160,170,180,190]})\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max=MinMaxScaler()\n",
    "min_max.fit_transform(df[['Height']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution: The Unit Vector technique, also known as Normalization or Vector Scaling, is a feature scaling method that transforms the data into unit vectors. This technique scales each feature such that the Euclidean norm (L2 norm) of the vector equals 1. This is particularly useful when you want to ensure that all features contribute equally to the distance computations, which is important for algorithm like k-NN, SVM, and PCA.\n",
    "\n",
    "Difference\n",
    "\n",
    "1. Unit Vector Scaling:\n",
    "    * Transform data such that the Euclidean norm(L2 norm) of the feature vector is 1.\n",
    "    * Each data point is scaled individually.\n",
    "    * usefull for algorithms that compute distances or dot products.\n",
    "\n",
    "2. Min-Max Scaling:\n",
    "    * Transform data to fit within a specific range(e.g., 0 to 1 or -1 to 1)\n",
    "    * Scales data based on the minimum and maximum values of the feature across the dataset.\n",
    "    * Useful for algorithms that require data within a specific range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>29.03</td>\n",
       "      <td>5.92</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>27.18</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>22.67</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>17.82</td>\n",
       "      <td>1.75</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>18.78</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Thur</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     total_bill   tip     sex smoker   day    time  size\n",
       "0         16.99  1.01  Female     No   Sun  Dinner     2\n",
       "1         10.34  1.66    Male     No   Sun  Dinner     3\n",
       "2         21.01  3.50    Male     No   Sun  Dinner     3\n",
       "3         23.68  3.31    Male     No   Sun  Dinner     2\n",
       "4         24.59  3.61  Female     No   Sun  Dinner     4\n",
       "..          ...   ...     ...    ...   ...     ...   ...\n",
       "239       29.03  5.92    Male     No   Sat  Dinner     3\n",
       "240       27.18  2.00  Female    Yes   Sat  Dinner     2\n",
       "241       22.67  2.00    Male    Yes   Sat  Dinner     2\n",
       "242       17.82  1.75    Male     No   Sat  Dinner     2\n",
       "243       18.78  3.00  Female     No  Thur  Dinner     2\n",
       "\n",
       "[244 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for example\n",
    "import seaborn as sns\n",
    "df=sns.load_dataset('tips')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998238</td>\n",
       "      <td>0.059342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.987357</td>\n",
       "      <td>0.158512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.986407</td>\n",
       "      <td>0.164323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.990372</td>\n",
       "      <td>0.138435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.989395</td>\n",
       "      <td>0.145251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.979834</td>\n",
       "      <td>0.199815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.997304</td>\n",
       "      <td>0.073385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.996131</td>\n",
       "      <td>0.087881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.995213</td>\n",
       "      <td>0.097734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0.987480</td>\n",
       "      <td>0.157744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     total_bill       tip\n",
       "0      0.998238  0.059342\n",
       "1      0.987357  0.158512\n",
       "2      0.986407  0.164323\n",
       "3      0.990372  0.138435\n",
       "4      0.989395  0.145251\n",
       "..          ...       ...\n",
       "239    0.979834  0.199815\n",
       "240    0.997304  0.073385\n",
       "241    0.996131  0.087881\n",
       "242    0.995213  0.097734\n",
       "243    0.987480  0.157744\n",
       "\n",
       "[244 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "pd.DataFrame(normalize(df[['total_bill','tip']]),columns=['total_bill','tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction in data analysis and machine learning. It identifies patterns and correlations in data and expresses them in terms of principal components. These components are linear combinations of the original features and are ordered by the amount of variance they explain in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Math  Science  English  History\n",
      "0    85       90       78       92\n",
      "1    70       75       80       85\n",
      "2    90       88       85       95\n",
      "3    60       70       65       80\n",
      "4    75       80       78       88\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Math': [85, 70, 90, 60, 75],\n",
    "    'Science': [90, 75, 88, 70, 80],\n",
    "    'English': [78, 80, 85, 65, 78],\n",
    "    'History': [92, 85, 95, 80, 88]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Step 1- Apply standard scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "df=pd.DataFrame(scaler.fit_transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.25       1.21106216 1.0406298  1.24793244]\n",
      " [1.21106216 1.25       0.88958814 1.19925522]\n",
      " [1.0406298  0.88958814 1.25       1.05746048]\n",
      " [1.24793244 1.19925522 1.05746048 1.25      ]]\n"
     ]
    }
   ],
   "source": [
    "##Step 2- Find Covariance Matrix\n",
    "import numpy as np\n",
    "covariance_matrix=np.cov([df.iloc[:,0],df.iloc[:,1],df.iloc[:,2],df.iloc[:,3]])\n",
    "print(covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Step 3 - Find EV and EVs\n",
    "eigen_values,eigen_vector=np.linalg.eig(covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.58319009e+00, 3.91440281e-01, 1.25010273e-03, 2.41195223e-02])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigen_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_index = np.argsort(eigen_values)[::-1]\n",
    "sorted_eigenvectors = eigen_vector[:, sorted_index]\n",
    "sorted_eigenvalues = eigen_values[sorted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the top 2 eigen_vectors\n",
    "\n",
    "pc = sorted_eigenvectors[:, 0:2] ##Trnasformation is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.51934128,  0.1573041 ],\n",
       "       [ 0.49848209,  0.50383412],\n",
       "       [ 0.46006901, -0.84276671],\n",
       "       [ 0.5197468 ,  0.10559766]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df=np.dot(df.iloc[:,0:4],pc)\n",
    "new_df=pd.DataFrame(transformed_df,columns=['PC1','PC2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.507392</td>\n",
       "      <td>0.735973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.762231</td>\n",
       "      <td>-0.877696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.402684</td>\n",
       "      <td>-0.154817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.115379</td>\n",
       "      <td>0.453079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.032466</td>\n",
       "      <td>-0.156540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PC1       PC2\n",
       "0  1.507392  0.735973\n",
       "1 -0.762231 -0.877696\n",
       "2  2.402684 -0.154817\n",
       "3 -3.115379  0.453079\n",
       "4 -0.032466 -0.156540"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is relationship between PCA and Feature Extraction and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) and Feature Extraction are closely related in the context of data preprocessing and dimensionality reduction.\n",
    "\n",
    "Feature Extraction involves transforming the original data into a new set of features that are usually more informative and less redundant. This is often done to improve the performance of machine learning algorithms.\n",
    "\n",
    "PCA is a popular technique used for feature extraction because it transforms the original features into a new set of uncorrelated features called principal components. These components are ordered such that the first few retain most of the variance present in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Your are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating and delivery time, Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        ],\n",
       "       [0.46524939, 0.        , 0.        ],\n",
       "       [0.11474516, 1.        , 0.46666667],\n",
       "       [0.95884437, 0.4       , 0.13333333],\n",
       "       [0.80921232, 0.6       , 0.26666667],\n",
       "       [0.98500954, 0.2       , 0.06666667],\n",
       "       [0.24448078, 0.4       , 0.8       ],\n",
       "       [0.17279913, 0.2       , 0.4       ],\n",
       "       [0.        , 0.8       , 0.8       ],\n",
       "       [0.36713001, 0.6       , 0.73333333]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df1=pd.DataFrame({'Price':np.random.randint(1000,5000,10),'Rating':np.random.randint(1,10,10),'Delivery_time':np.random.randint(0,24,10)})\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max=MinMaxScaler()\n",
    "min_max.fit_transform(df1[['Price','Rating','Delivery_time']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to  predict the stock prices. The dataset contains many features, such as company financial data and market tends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         PC1       PC2\n",
      "0   1.622064 -2.599418\n",
      "1  -0.771510 -0.715053\n",
      "2  -2.055693 -1.826946\n",
      "3   1.674217  0.183205\n",
      "4   0.372914 -0.995896\n",
      "5  -0.600963 -0.936548\n",
      "6   0.263886  0.422697\n",
      "7   1.588595  1.079853\n",
      "8  -1.553541  0.434139\n",
      "9  -0.294694 -1.052425\n",
      "10  0.357407  1.353378\n",
      "11  1.365966  0.847220\n",
      "12  0.771800  0.439616\n",
      "13 -1.753819  0.159753\n",
      "14 -1.444855  0.323594\n",
      "15 -1.095296 -0.325889\n",
      "16 -1.197692  1.387225\n",
      "17  0.466522 -0.889782\n",
      "18 -1.511520  2.671004\n",
      "19 -0.248052 -1.171143\n",
      "20  0.436564  0.723473\n",
      "21 -1.623755  1.619971\n",
      "22  0.534179  1.426447\n",
      "23 -0.519072 -1.441946\n",
      "24  0.348845 -1.085610\n",
      "25 -1.066210  0.064252\n",
      "26  0.950073 -0.825331\n",
      "27 -1.941320  0.757462\n",
      "28  2.738711  0.657608\n",
      "29 -0.903608  1.909154\n",
      "30  0.332591 -1.497301\n",
      "31  0.065533 -0.337640\n",
      "32  1.980139 -0.390379\n",
      "33  1.320662  0.957730\n",
      "34 -1.865564  0.436537\n",
      "35  0.187364 -0.385535\n",
      "36 -0.353355 -1.589836\n",
      "37 -1.746057 -0.298367\n",
      "38  0.169234 -0.424677\n",
      "39 -2.760276 -0.127267\n",
      "40 -0.098833  2.900864\n",
      "41  1.647058 -0.830271\n",
      "42  0.378666 -0.969833\n",
      "43  0.005682 -1.345063\n",
      "44 -1.884579 -1.375663\n",
      "45  0.939872  0.908711\n",
      "46  1.087513  0.162591\n",
      "47  1.150782  0.237578\n",
      "48  3.034587  0.806216\n",
      "49  1.498835  0.567542\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a sample dataset\n",
    "np.random.seed(42) # For reproducibility\n",
    "data={\n",
    "    'Revenue': np.random.uniform(100,1000,50),\n",
    "    'Profit': np.random.uniform(10,100,50),\n",
    "    'Expenses': np.random.uniform(50,500,50),\n",
    "    'Market Share':np.random.uniform(1,50,50),\n",
    "    'Stock Price':np.random.uniform(20,200,50),\n",
    "    'Employee Count':np.random.randint(50,1000,50),\n",
    "    'Customer Satisfaction': np.random.uniform(1,10,50)\n",
    "}\n",
    "\n",
    "##Create DataFrame\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "#Standardize the data\n",
    "scaler=StandardScaler()\n",
    "df = scaler.fit_transform(df)\n",
    "\n",
    "#Perform PCA\n",
    "pca = PCA(n_components=2) \n",
    "pc=pca.fit_transform(df)\n",
    "\n",
    "new_df=pd.DataFrame(data=pc,columns=['PC1','PC2'])\n",
    "print(new_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explained Variance Ratio:\n",
      " [0.25088179 0.18666339]\n"
     ]
    }
   ],
   "source": [
    "# Explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(\"\\nExplained Variance Ratio:\\n\", explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values:[1,5,10,15,20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.DataFrame({'f1':[1,5,10,15,20]})\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler(feature_range=(-1,1))\n",
    "scaler.fit_transform(df2[['f1']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features:[height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         PC1       PC2\n",
      "0   0.252524  0.676745\n",
      "1  -2.197804 -1.217594\n",
      "2   0.897272  0.587865\n",
      "3  -1.226227  0.411631\n",
      "4   2.054044 -1.790537\n",
      "..       ...       ...\n",
      "95  0.038077 -0.089681\n",
      "96  0.501304  0.298877\n",
      "97  0.899993 -1.943611\n",
      "98  1.423279 -1.278581\n",
      "99  1.427504 -1.420649\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate sample data\n",
    "num_samples = 100\n",
    "data = {\n",
    "    'Height': np.random.uniform(150, 200, num_samples),  # Height in cm\n",
    "    'Weight': np.random.uniform(50, 100, num_samples),   # Weight in kg\n",
    "    'Age': np.random.randint(18, 80, num_samples),        # Age in years\n",
    "    'Gender': np.random.randint(0, 2, num_samples),       # Gender (0: Male, 1: Female)\n",
    "    'Blood Pressure': np.random.uniform(70, 180, num_samples)  # Blood Pressure in mmHg\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#Standardize the data\n",
    "scaler=StandardScaler()\n",
    "df = scaler.fit_transform(df)\n",
    "\n",
    "#Perform PCA\n",
    "pca = PCA(n_components=2) \n",
    "pc=pca.fit_transform(df)\n",
    "\n",
    "new_df=pd.DataFrame(data=pc,columns=['PC1','PC2'])\n",
    "print(new_df)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
