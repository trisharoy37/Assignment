{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Ans: \n",
    "\n",
    "**Simple Linear Regression**\n",
    "In simple linear regression, we attempt to model the relationship between a dependent variable and a single independent variable using a linear equation. The goal is to find the best-fitting line that minimizes the distance between the predicted values and the actual values.\n",
    "Example:\n",
    "- Dependent Variable: House Price\n",
    "- Independent Variable: House Size (square feet)\n",
    "We want to predict the price of a house based on it's size.\n",
    "\n",
    "**Multiple Linear Regression**\n",
    "Multiple linear regression extends simple linear regression by incorporating multiple independent variables to predict a dependent variable. This allows for more complex relationships and potentially more accurate predictions.\n",
    "Example:\n",
    "- Dependent Variable: Car Price\n",
    "- Independent Variables:\n",
    "    1. Mileage\n",
    "    2. Age\n",
    "    3. Engine Size\n",
    "    4. Horsepower\n",
    "We want to predict the price of car based on its mileage, age, engine size, and horsepower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?**\n",
    "\n",
    "Ans:\n",
    "\n",
    "Assupmtions of Linear Regression:\n",
    "1. Linearity: The relationship between the dependent variable(Y) and independent variables(X) is linear. This means that a change in X results in a proportional change in Y.\n",
    "    - Checking:\n",
    "    - Scatter plots: Visualize the relationship between Y and each X. A linear pattern suggests linearity\n",
    "    - Residual plots: Plot the residuals against the predicted values. A random scatter indicates linearity.\n",
    "\n",
    "2. Homoscedasticity: The variance of the errors is constant across all values of the independent variables. This means that the spread of the residuals is consistent.\n",
    "    - Checking:\n",
    "    - Residual plots: Look for a consistent spread of residuals across the range of predicted values.\n",
    "    - Breusch-Pagan test: This statistical test formally tests for heteroscedasticity.\n",
    "\n",
    "3. Independence of Errors: The errors(residuals) are independent of each other. This means that the error in one observation does not influence the error in another observation.\n",
    "    - Checking:\n",
    "    - Durbin-Watson test: This statistical test checks for autocorrelation, which violates the independence assumption. A value close to 2 indicates independence.\n",
    "\n",
    "4. Normality of Errors: The errors are normally distributed. This assumption is important for hypothesis testing and confidence interval calculations.\n",
    "    - Checking: \n",
    "    - Histogram of residuals: A bell-shaped curve suggests normality.\n",
    "    - Q-Q plot: Compare the quantiles of the residuals to the quantiles of a normal distribution. A straight line indicates normality.\n",
    "\n",
    "5. No multicollinearity: The independent variables are not highly correlated with each other. Multicollinearity can make it difficult to estimate the individual effects of the independent variables on the dependent variable.\n",
    "    - Checking:\n",
    "    - Correlation matrix: Calculate the correlation coefficients between the independent variables. High correlations suggest multicollinearity.\n",
    "    - Variance Inflation Factor(VIF): A VIF greater than 10 indicates high multicollinearity.\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Ans: In a linear regression model, the equation of the line is typically represented as:\n",
    "\n",
    "Y = b0 + b1*X\n",
    "\n",
    "Where:\n",
    "- Y: Dependent variable (the outcome we want to predict)\n",
    "- X: Independent variable (the predictor variable)\n",
    "- b0: Intercept\n",
    "- b1: Slope\n",
    "\n",
    "*Intercept (b0)*\n",
    "\n",
    "The intercept represents the predicted value of Y when X is equal to 0. It's the starting point of the line.\n",
    "\n",
    "*Slope (b1)*\n",
    "\n",
    "The slope represents the rate of change of Y with respect to X. In other words, it tells us how much Y changes for a one-unit increase in X.\n",
    "\n",
    "**Real-world example: Predicting House Prices**\n",
    "\n",
    "*Let's say we want to predict the price of a house based on its square footage. After running a linear regression, we get the following equation:*\n",
    "\n",
    "Price = 50000 + 100 * SquareFootage\n",
    "Interpretation:\n",
    "\n",
    "Intercept (b0 = 50000): This means that a house with 0 square footage (which is unrealistic) would have a predicted price of $50,000. This might represent a base value like land value or minimum construction costs.\n",
    "Slope (b1 = 100): This means that for every additional square foot of living space, the predicted price of the house increases by $100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q4. Explain the concept of gradient descent. How is it used in machine learning?**\n",
    "Ans: Gradient descent is an optimization algorithm used to find the minimum of a function. In machine learning, it's used to minimize the ost function, which measures how well a model's predictions match the actual values.\n",
    "\n",
    "*How it works:*\n",
    "1. Initialize Parameters:\n",
    "    - Start with random initial values for the model's parameters (weights and biases).\n",
    "2. Calculate the Gradient:\n",
    "    - Compute the gradient of the cost function with respect to the parameters. The gradient indicates the direction of steepest ascent.\n",
    "3. Update Parameters:\n",
    "    - Update the parameters in the opposite direction of the gradient, multiplied by a learning rate:\n",
    "    ***new_parameter = old_parameter - learning_rate * gradient***\n",
    "4. Repeat:\n",
    "    - Iterate steps 2 and 3 until the gradient becomes very small, indicating that we've reached a minimum point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?**\n",
    "\n",
    "Ans: **Multiple linear regression** is a statistical method used to model the relationship between a dependent variable  and two or more independent variables. It extends the concept of simple linear regression, which involves only one independent variable.\n",
    "\n",
    "The general equation for multiple linear regression is:\n",
    "\n",
    "Y = β₀ + β₁X₁ + β₂X₂ + ... + βₚXₚ + ε\n",
    "\n",
    "Where:\n",
    "- Y: Dependent variable\n",
    "- X₁, X₂, ..., Xₚ: Independent variables\n",
    "- β₀: Intercept\n",
    "- β₁, β₂, ..., βₚ: Coefficients for each independent variable\n",
    "- ε: Error term\n",
    "\n",
    "Key Differences from Simple Linear Regression:\n",
    "\n",
    "1. Number of Independent Variables:\n",
    "    - Simple linear regression: One independent variable\n",
    "    - Multiple linear regression: Two or more independent variables\n",
    "2. Model Complexity:\n",
    "    - Simple linear regression: A linear relationship between two variables\n",
    "    - Multiple linear regression: A more complex relationship involving multiple variables,           potentially capturing non-linear effects and interactions.\n",
    "3. Model Interpretation:\n",
    "    - Simple linear regression: The slope coefficient indicates the change in the dependent variable for a unit change in the independent variable.\n",
    "    - Multiple linear regression: The coefficients for each independent variable represent the change in the dependent variable for a unit change in that specific independent variable, holding other variables constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?**\n",
    "\n",
    "Ans: Multicollinearity occurs when two or more independent variables in a regression model are highly correlated with each other.\n",
    "\n",
    "Detecting Multicollinearity:\n",
    "1. Correlation Matrix:\n",
    "    - Calculate the correlation coefficients between all pairs of independent variables.   \n",
    "    - High correlation coefficients (e.g., above 0.7 or 0.8) indicate potential multicollinearity.\n",
    "2. Variance Inflation Factor (VIF):\n",
    "    - VIF measures the extent to which the variance of a coefficient estimate is inflated due to multicollinearity.\n",
    "    - A VIF greater than 10 suggests high multicollinearity.\n",
    "3. Tolerance:\n",
    "    - Tolerance is the reciprocal of VIF.\n",
    "    - A tolerance value less than 0.1 indicates high multicollinearity.\n",
    "\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "1. Feature Engineering:\n",
    "    - Combine highly correlated variables into a single variable or create a new variable that captures the underlying relationship.\n",
    "2. Principal Component Analysis (PCA):\n",
    "    - Reduce the dimensionality of the data by creating uncorrelated linear combinations of the original variables.\n",
    "3. Remove Redundant Variables:\n",
    "    - If two variables are highly correlated, remove one of them from the model.\n",
    "4. Ridge Regression:\n",
    "    - This technique adds a penalty term to the regression equation to shrink the coefficients and reduce the impact of multicollinearity.\n",
    "5. Lasso Regression:\n",
    "    - This technique can automatically select a subset of relevant variables, potentially reducing the impact of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q7. Describe the polynomial regression model. How is it different from linear regression?**\n",
    "\n",
    "Ans: Polynomial Regression is a form of regression analysis in which the relationship between the independent variable (x) and the dependent variable (y) is modeled as an nth degree polynomial. This allows for more complex, non-linear relationships between the variables.   \n",
    "\n",
    "Mathematical Representation:\n",
    "\n",
    "y = β₀ + β₁x + β₂x² + ... + βₙxⁿ + ε\n",
    "\n",
    "Where:\n",
    "\n",
    "- y: Dependent variable\n",
    "- x: Independent variable\n",
    "- β₀, β₁, ..., βₙ: Coefficients\n",
    "- ε: Error term\n",
    "- n: Degree of the polynomial\n",
    "\n",
    "Key Differences from Linear Regression:\n",
    "\n",
    "1. Relationship:\n",
    "    - Linear Regression: Assumes a linear relationship between the variables.   \n",
    "    - Polynomial Regression: Allows for non-linear relationships by introducing polynomial terms (x², x³, etc.) into the model.   \n",
    "2. Model Complexity:\n",
    "    - Linear Regression: Simpler model, often suitable for linear relationships.   \n",
    "    - Polynomial Regression: More flexible model, capable of capturing complex patterns in data.   \n",
    "3. Overfitting:\n",
    "    - Polynomial Regression: Prone to overfitting, especially with higher-degree polynomials. Careful model selection and regularization techniques are crucial.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Ans: Polynomial Regression is a form of regression analysis in which the relationship between the independent variable (x) and the dependent variable (y) is modeled as an nth degree polynomial. This allows for more complex, non-linear relationships between the variables.   \n",
    "\n",
    "Mathematical Representation:\n",
    "\n",
    "y = β₀ + β₁x + β₂x² + ... + βₙxⁿ + ε\n",
    "\n",
    "Where:\n",
    "\n",
    "- y: Dependent variable\n",
    "- x: Independent variable\n",
    "- β₀, β₁, ..., βₙ: Coefficients\n",
    "- ε: Error term\n",
    "- n: Degree of the polynomial\n",
    "\n",
    "Key Differences from Linear Regression:\n",
    "\n",
    "1. Relationship:\n",
    "    - Linear Regression: Assumes a linear relationship between the variables.   \n",
    "    - Polynomial Regression: Allows for non-linear relationships by introducing polynomial terms (x², x³, etc.) into the model.   \n",
    "2. Model Complexity:\n",
    "    - Linear Regression: Simpler model, often suitable for linear relationships.   \n",
    "    - Polynomial Regression: More flexible model, capable of capturing complex patterns in data.   \n",
    "3. Overfitting:\n",
    "    - Polynomial Regression: Prone to overfitting, especially with higher-degree polynomials. Careful model selection and regularization techniques are crucial.   \n",
    "\n",
    "When to Use Polynomial Regression:\n",
    "- When the relationship between variables is non-linear.\n",
    "- When the data exhibits curvature or trends that cannot be captured by a linear model.\n",
    "- When you need to model complex patterns in the data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
